
@article{tran_deep_2017,
  title = {Deep {{Probabilistic Programming}}},
  url = {http://arxiv.org/abs/1701.03757},
  abstract = {We propose Edward, a Turing-complete probabilistic programming language. Edward builds on two compositional representations---random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation, to variational inference, to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, on a benchmark logistic regression task, Edward is at least 35x faster than Stan and PyMC3.},
  timestamp = {2017-03-16T11:10:02Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.03757},
  primaryClass = {cs, stat},
  author = {Tran, Dustin and Hoffman, Matthew D. and Saurous, Rif A. and Brevdo, Eugene and Murphy, Kevin and Blei, David M.},
  urldate = {2017-01-17},
  date = {2017-01-13},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Computer Science - Programming Languages,Statistics - Computation,Statistics - Machine Learning},
  file = {arXiv\:1701.03757 PDF:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/UQ3WRD3J/Tran et al. - 2017 - Deep Probabilistic Programming.pdf:application/pdf;arXiv\:1701.03757 PDF:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/W7MN8G8D/Tran et al. - 2017 - Deep Probabilistic Programming.pdf:application/pdf;arXiv.org Snapshot:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/FMQPZ4Z9/1701.html:text/html;arXiv.org Snapshot:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/K8BXGMVD/1701.html:text/html}
}

@article{david_m._blei_build_2014,
  title = {Build, {{Compute}}, {{Critique}}, {{Repeat}}: {{Data Analysis}} with {{Latent Variable Models}}},
  volume = {1},
  url = {http://dx.doi.org/10.1146/annurev-statistics-022513-115657},
  doi = {10.1146/annurev-statistics-022513-115657},
  shorttitle = {Build, {{Compute}}, {{Critique}}, {{Repeat}}},
  abstract = {We survey latent variable models for solving data-analysis problems. A latent variable model is a probabilistic model that encodes hidden patterns in the data. We uncover these patterns from their conditional distribution and use them to summarize data and form predictions. Latent variable models are important in many fields, including computational biology, natural language processing, and social network analysis. Our perspective is that models are developed iteratively: We build a model, use it to analyze data, assess how it succeeds and fails, revise it, and repeat. We describe how new research has transformed these essential activities. First, we describe probabilistic graphical models, a language for formulating latent variable models. Second, we describe mean field variational inference, a generic algorithm for approximating conditional distributions. Third, we describe how to use our analyses to solve problems: exploring the data, forming predictions, and pointing us in the direction of improved models.},
  timestamp = {2017-02-09T13:09:53Z},
  number = {1},
  journaltitle = {Annual Review of Statistics and Its Application},
  author = {{David M. Blei}},
  urldate = {2017-02-09},
  date = {2014},
  pages = {203--232},
  file = {Full Text PDF:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/Z2NPG2II/2014 - Build, Compute, Critique, Repeat Data Analysis wi.pdf:application/pdf}
}

@inproceedings{kingma_semi-supervised_2014,
  title = {Semi-Supervised Learning with Deep Generative Models},
  url = {http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models},
  timestamp = {2017-03-24T08:25:11Z},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kingma, Diederik P. and Mohamed, Shakir and Rezende, Danilo Jimenez and Welling, Max},
  urldate = {2017-03-24},
  date = {2014},
  pages = {3581--3589},
  file = {kingma_rezende_deep_generative_models.pdf:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/8SWK2ZH3/kingma_rezende_deep_generative_models.pdf:application/pdf}
}

@article{box_discrimination_1967-1,
  title = {Discrimination among {{Mechanistic Models}}},
  volume = {9},
  issn = {0040-1706},
  doi = {10.2307/1266318},
  abstract = {This paper is concerned with research, the object of which is to discover the mechanism for a particular phenomenon leading to a specific mathematical model. Such investigations are distinguished from those in which the object is merely to estimate the output y of a process over a range of values of the input ξ 1, ξ 2,⋯, ξ k. Frequently, a number of possible mechanisms are suggested from theoretical considerations leading to a number of different mathematical models. To discriminate among these a sequential procedure is developed in which calculations made after each experiment determine the most discriminatory process conditions for use in the next experiment. The method is illustrated with examples.},
  timestamp = {2017-04-24T14:27:08Z},
  eprinttype = {jstor},
  eprint = {1266318},
  number = {1},
  journaltitle = {Technometrics},
  author = {Box, G. E. P. and Hill, W. J.},
  date = {1967},
  pages = {57--71},
  file = {JSTOR Full Text PDF:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/C8D8225C/Box and Hill - 1967 - Discrimination among Mechanistic Models.pdf:application/pdf}
}

@article{box_experimental_1965-1,
  title = {The {{Experimental Study}} of {{Physical Mechanisms}}},
  volume = {7},
  issn = {0040-1706},
  doi = {10.2307/1266125},
  abstract = {This paper is concerned with the dual problem of generating and analyzing data in experimental investigations in which the goal is to develop a suitable mechanistic model. The problem is first distinguished from that of response surface methodology. With regard to the analysis of data, topics that are discussed include the behavior of estimated constants with an inadequate model, a diagnostic technique for model-building, and the importance of visual scrutiny of data. With regard to the generation of data, the concept of placing a model in jeopardy is discussed. Designs for model discrimination and for parameter estimation are considered.},
  timestamp = {2017-04-24T14:27:10Z},
  eprinttype = {jstor},
  eprint = {1266125},
  number = {1},
  journaltitle = {Technometrics},
  author = {Box, G. E. P. and Hunter, William G.},
  date = {1965},
  pages = {23--42},
  file = {JSTOR Full Text PDF:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/52RPZEW5/Box and Hunter - 1965 - The Experimental Study of Physical Mechanisms.pdf:application/pdf}
}

@article{box_useful_1962-1,
  title = {A {{Useful Method}} for {{Model}}-{{Building}}},
  volume = {4},
  issn = {0040-1706},
  doi = {10.2307/1266570},
  abstract = {A simple technique useful in iterative model-building is described. In this a statistical analysis is applied to the estimated parameters rather than to the observations directly. This mode of procedure pinpoints the source and nature of possible inadequacies and these, interacting with the experimenter's special knowledge of the problem, can suggest what modifications are necessary for improvement. The cycle may be repeated till an adequate model, which should of course be checked by additional critical experiments, is found.},
  timestamp = {2017-04-24T14:27:13Z},
  eprinttype = {jstor},
  eprint = {1266570},
  number = {3},
  journaltitle = {Technometrics},
  author = {Box, G. E. P. and Hunter, William G.},
  date = {1962},
  pages = {301--318},
  file = {JSTOR Full Text PDF:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/QMSGD4MZ/Box and Hunter - 1962 - A Useful Method for Model-Building.pdf:application/pdf}
}

@article{box_sampling_1980-1,
  title = {Sampling and {{Bayes}}' {{Inference}} in {{Scientific Modelling}} and {{Robustness}}},
  volume = {143},
  issn = {0035-9238},
  doi = {10.2307/2982063},
  abstract = {Scientific learning is an iterative process employing Criticism and Estimation. Correspondingly the formulated model factors into two complementary parts--a predictive part allowing model criticism, and a Bayes posterior part allowing estimation. Implications for significance tests, the theory of precise measurement and for ridge estimates are considered. Predictive checking functions for transformation, serial correlation, bad values, and their relation with Bayesian options are considered. Robustness is seen from a Bayesian viewpoint and examples are given. For the bad value problem a comparison with M estimators is made.},
  timestamp = {2017-04-24T14:27:55Z},
  eprinttype = {jstor},
  eprint = {2982063},
  number = {4},
  journaltitle = {Journal of the Royal Statistical Society. Series A (General)},
  author = {Box, George E. P.},
  date = {1980},
  pages = {383--430},
  file = {JSTOR Full Text PDF:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/JBNXH93T/Box - 1980 - Sampling and Bayes' Inference in Scientific Modell.pdf:application/pdf}
}

@article{box_science_1976-1,
  title = {Science and {{Statistics}}},
  volume = {71},
  issn = {0162-1459},
  doi = {10.2307/2286841},
  abstract = {Aspects of scientific method are discussed: In particular, its representation as a motivated iteration in which, in succession, practice confronts theory, and theory, practice. Rapid progress requires sufficient flexibility to profit from such confrontations, and the ability to devise parsimonious but effective models, to worry selectively about model inadequacies and to employ mathematics skillfully but appropriately. The development of statistical methods at Rothamsted Experimental Station by Sir Ronald Fisher is used to illustrate these themes.},
  timestamp = {2017-04-24T14:28:05Z},
  eprinttype = {jstor},
  eprint = {2286841},
  number = {356},
  journaltitle = {Journal of the American Statistical Association},
  author = {Box, George E. P.},
  date = {1976},
  pages = {791--799},
  file = {JSTOR Full Text PDF:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/TFQQFSEK/Box - 1976 - Science and Statistics.pdf:application/pdf}
}

@article{salvatier_probabilistic_2015,
  title = {Probabilistic {{Programming}} in {{Python}} Using {{PyMC}}},
  url = {http://arxiv.org/abs/1507.08050},
  abstract = {Probabilistic programming (PP) allows flexible specification of Bayesian statistical models in code. PyMC3 is a new, open-source PP framework with an intutive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models. It features next-generation Markov chain Monte Carlo (MCMC) sampling algorithms such as the No-U-Turn Sampler (NUTS; Hoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane, 1987). Probabilistic programming in Python confers a number of advantages including multi-platform compatibility, an expressive yet clean and readable syntax, easy integration with other scientific libraries, and extensibility via C, C++, Fortran or Cython. These features make it relatively straightforward to write and use custom statistical distributions, samplers and transformation functions, as required by Bayesian analysis.},
  timestamp = {2017-04-25T08:32:16Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.08050},
  primaryClass = {stat},
  author = {Salvatier, John and Wiecki, Thomas and Fonnesbeck, Christopher},
  urldate = {2017-04-25},
  date = {2015-07-29},
  keywords = {Statistics - Computation},
  file = {arXiv\:1507.08050 PDF:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/QTNN4IEH/Salvatier et al. - 2015 - Probabilistic Programming in Python using PyMC.pdf:application/pdf;arXiv.org Snapshot:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/JKSEAVS3/1507.html:text/html}
}

@article{carpenter_stan_2017,
  title = {Stan : {{A Probabilistic Programming Language}}},
  volume = {76},
  issn = {1548-7660},
  url = {http://www.jstatsoft.org/v76/i01/},
  doi = {10.18637/jss.v076.i01},
  shorttitle = {\emph{Stan}},
  timestamp = {2017-04-25T08:44:03Z},
  langid = {english},
  number = {1},
  journaltitle = {Journal of Statistical Software},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  urldate = {2017-04-25},
  date = {2017},
  file = {v76i01.pdf:/home/john/.mozilla/firefox/tpi3ht8v.default/zotero/storage/FTI3MHMC/v76i01.pdf:application/pdf}
}


