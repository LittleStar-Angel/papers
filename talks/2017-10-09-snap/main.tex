\documentclass[10pt,
               xcolor={usenames,dvipsnames},
               hyperref={colorlinks,linktoc=all,citecolor=Plum,linkcolor=MidnightBlue,urlcolor=MidnightBlue},noamssymb]{beamer}
\input{preamble/preamble}
\input{preamble/preamble_acronyms}
\input{preamble/preamble_math}
\input{preamble/preamble_tikz}

\usepackage{subfigure}
\definecolor{light}{RGB}{199, 153, 199}
\definecolor{dark}{RGB}{143, 39, 143}
\definecolor{gray80}{gray}{0.8}

\usepackage{natbib}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{itemize items}[circle]
\setbeamercolor{itemize item}{fg=black!67}
\setbeamercolor*{enumerate item}{fg=black!67}
\setbeamercolor*{enumerate subitem}{fg=black!67}
\setbeamercolor*{enumerate subsubitem}{fg=black!67}

\definecolor{charcoal}{HTML}{222222}
\definecolor{snow}{HTML}{F9F9F9}

\setbeamercolor{background canvas}{bg=white}
\setbeamercolor{normal text}{fg=charcoal}
\setbeamercolor{structure}{fg=charcoal}

\newenvironment{changemargin}[1]{
  \begin{list}{}{
    \setlength{\topsep}{0pt}
    \setlength{\leftmargin}{#1}
    \setlength{\rightmargin}{#1}
    \setlength{\listparindent}{\parindent}
    \setlength{\itemindent}{\parindent}
    \setlength{\parsep}{\parskip}
  }
  \item[]}{\end{list}}

\title{}
\begin{document}

\begin{frame}[plain,t]
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=0.50cm, yshift=-3.00cm, anchor=north west] at (current page.north west) {
    \begin{tabular}{l}
    {\Large\bf Deep Probabilistic Programming}\\[2ex]
    {\large }\\[4ex]
    Dustin Tran\\
    Columbia University, OpenAI \\[4ex]
    \end{tabular}
  };
  \node [xshift=-1.50cm, yshift=3.00cm, anchor=mid east] at (current page.south
  east) {
\includegraphics[width=0.25\textwidth]{img/edward.png}
  };
  \node [xshift=-1.25cm, yshift=0.5cm, anchor=mid east] at (current page.south
  east) {
    \includegraphics[width=0.22\textwidth]{img/columbia.pdf}
  };
  \node [xshift=1.25cm, yshift=-0.9cm, anchor=mid east] at (current page.south
  east) {
    \includegraphics[width=0.40\textwidth]{img/openai.pdf}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}[plain]
\footnotesize
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=22mm]{img/alp.png} &
\includegraphics[width=22mm]{img/adji.jpg} &
\includegraphics[width=22mm]{img/dawen.jpg} \\
Alp Kucukelbir & Adji Dieng & Dawen Liang \\
\end{tabular}

\vspace{-2ex}

\begin{tabular}{cccc}
\includegraphics[width=22mm]{img/eugene.jpg} &
\includegraphics[width=22mm]{img/maja.png} &
\includegraphics[width=22mm]{img/matt.jpg} &
\includegraphics[width=22mm]{img/ranganath.jpg} \\
Eugene Brevdo & Maja Rudolph & Matt Hoffman & Rajesh Ranganath \\
\end{tabular}

\vspace{-2ex}

\begin{tabular}{cccc}
\includegraphics[width=20mm]{img/gelman.png} &
\includegraphics[width=20mm]{img/blei.jpg} &
\includegraphics[width=20mm]{img/kevin.jpg} &
\includegraphics[width=20mm]{img/rif.png}\\
Andrew Gelman & David Blei & Kevin Murphy & Rif Saurous\\
\end{tabular}
\end{center}
\end{frame}

\begin{frame}[plain,t]
\vspace{2ex}
\begin{center}
\includegraphics[width=0.9\textwidth]{img/taxis.png}
\\[2ex]
Exploratory analysis of 1.7M taxi trajectories, in Stan
\end{center}
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=-3.0cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [Kucukelbir+ 2017]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}[plain,t]
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=0.25cm, yshift=-1.5cm, anchor=north west] at (current
  page.north west) {
\includegraphics[width=0.6\textwidth]{img/lotka_volterra_plot.png}
  };
  \node [xshift=6.75cm, yshift=-2.0cm, anchor=north west] at (current
  page.north west) {
\includegraphics[width=0.5\textwidth]{img/lotka_volterra_inference.png}
  };
  \node [xshift=2.5cm, yshift=1.0cm, anchor=south west] at (current
  page.south west) {
Simulators of 100K time series in ecology, in Edward
  };
  \node [xshift=-2.25cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [Tran+ 2017]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}[plain,t]
\vspace{2.0cm}
\begin{center}
\includegraphics[width=0.37\textwidth]{img/dim_graphical_model.png}
\hfill
\includegraphics[width=0.45\textwidth]{img/dim_samples.png}
\\[1.5cm]
Generation \& compression of 10M colored 32x32 images, in Edward
\end{center}
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=-5.75cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [Tran+ 2017; fig from Van der Oord+ 2016]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}[plain,t]
\vspace{8ex}
\begin{center}
\begin{tabular}{@{~~~~~}l@{~~~~~}}
\toprule
A solid , spooky entertainment worthy of the price of a ticket . \\
Of all the Halloween 's , this is the most visually unappealing . \\[2ex]

Poetry in motion captured on film . \\
An imponderably stilted and self-consciously arty movie . \\[2ex]

Steers refreshingly clear of the usual cliches . \\
A profoundly stupid affair , populating its hackneyed and meanspirited
\\\quad
storyline with cardboard characters and performers who value cash
above\\\quad
credibility . \\[2ex]

A visual spectacle full of stunning images and effects . \\
An annoying orgy of excess and exploitation that has no point
and\\\quad goes nowhere . \\
\bottomrule
\end{tabular}
\\[5ex]
Sentiment transfer of unaligned pairs of 8.5K sentences, in Edward
\end{center}
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=-2.5cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [in preparation]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}[plain,t]
\begin{center}
\includegraphics[width=0.9\textwidth]{img/genetics.png}
\\[2ex]
Cause and effect of 1.6B genetic measurements, in Edward
\end{center}
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=-5.5cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [in preparation; fig from Gopalan+ 2017]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}
\frametitle{What is probabilistic programming?}
\textbf{Probabilistic programs reify models from mathematics to
physical objects.}
\begin{itemize}
\vspace{-2ex}
\item
Each model is equipped with memory (``bits'',
floating point, storage) and computation
(``flops'', scalability, communication).
\end{itemize}
\textbf{Anything you do lives in the world of probabilistic programming.}
\begin{itemize}
\item
Any computable model.

\gray{ex.
graphical models; neural networks; SVMs; stochastic processes.
}
\item
Any computable inference algorithm.

\gray{ex.
automated inference;
model-specific algorithms;
inference within inference (learning to learn).
}
\item
Any computable application.

\gray{ex.
exploratory analysis;
object recognition;
code generation;
causality.
}
\end{itemize}
\end{frame}

\begin{frame}
\begin{center}
{\Huge
\textit{\textbf{Simulation hypothesis.} \\
``The universe is a simulation from a computer program.''
}
\\[2ex]
{\Large
(Zuse, Bostrom, Schmidhuber, Musk)
}}
\end{center}
\end{frame}

\begin{frame}[plain,t]
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=-0.5cm, yshift=0.0cm, anchor=north west] at (current
  page.north west) {
\includegraphics[width=1.25\textwidth]{img/wood2015nips.png}
  };
  \node [xshift=-3.25cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [fig. from Frank Wood]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}
\frametitle{George E.P. Box (1919 - 2013)}
\begin{columns}
\begin{column}{0.5\textwidth}
    \begin{center}
     \includegraphics[width=\columnwidth]{img/box.jpg}
     \end{center}
\end{column}
\begin{column}{0.5\textwidth}
An iterative process for science:
\\[1ex]
\begin{enumerate}
\item Build a model of the science
\\[1ex]
\item Infer the model given data
\\[1ex]
\item Criticize the model given data
\end{enumerate}
\end{column}
\end{columns}
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=-8.0cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [Box \& Hunter 1962, 1965; Box \& Hill 1967; Box 1976, 1980]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}
\frametitle{Box's Loop}
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=-1cm, yshift=-2.00cm, anchor=north west] at (current page.north west) {
\includegraphics[width=1.4\textwidth]{img/model_infer_criticize.png}
  };
  \node [xshift=3.4cm, yshift=-8.0cm, anchor=north west] at (current page.north west) {
Edward is a library designed around this loop.
  };
  \node [xshift=0cm, yshift=-5.50cm, anchor=north west] at (current page.north west) {
  };
  \node [xshift=-4.0cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [Box 1976, 1980; Blei 2014]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}
\begin{center}
\vspace{-3.5ex}
\includegraphics[width=1.0\textwidth]{img/github.png}
\\[-1ex]
\includegraphics[width=1.0\textwidth]{img/forum.png}
\\[-3ex]
\includegraphics[width=1.0\textwidth]{img/gitter.png}
\\[2ex]
\end{center}
\text{
We have an active community of several thousand users \& many
contributors.
}
\end{frame}

\begin{frame}
\begin{center}
{\Large\bf How do we use Edward?}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Model}
A random variable $\mbx$ is an object parameterized by tensors
(multi-dimensional arrays) $\theta^*$.

\vspace{-1.0ex}
\includegraphics[height=0.20\textwidth]{img/random_variables.png}

It is equipped with methods such as \texttt{log\_prob()} and \texttt{sample()}.

Each random variable is associated to a tensor $\mbx^*$, $\mbx^*\sim p(\mbx\g\theta^*)$.

Mutable states let random variables condition on values that
change, e.g., discriminative models $p(\mby\g\mbx)$ and model
parameters $p(\mbx; \theta)$.
\end{frame}

\begin{frame}
\frametitle{Example: Beta-Bernoulli}
Consider a Beta-Bernoulli model,
\begin{align*}
p(\mbx, \theta) =
\operatorname{Beta}(\theta\g 1, 1)
\prod_{n=1}^{50} \operatorname{Bernoulli}(x_n\g \theta),
\end{align*}
where $\theta$
is a probability shared across 50 data points $\mbx\in\{0,1\}^{50}$.
\begin{center}
\vspace{-2ex}
\includegraphics[height=0.175\textwidth]{img/beta-bernoulli.png}
\end{center}
Fetching $\mbx$ from the graph generates a binary vector of $50$ elements.

All computation is represented on the graph, enabling us to leverage model structure during inference.
\end{frame}

\begin{frame}
\frametitle{Example: Bayesian neural network for classification}
\begin{center}
\includegraphics[height=0.3\textwidth]{img/bayesian_nn_graph.png}
\\[2.5ex]
\includegraphics[height=0.19\textwidth]{img/bayesian_nn_program.png}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Example: Variational Auto-Encoder for Binarized MNIST}
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=0.4cm, yshift=-0.8cm, anchor=north west] at (current page.north west) {
   \includegraphics[width=0.5\columnwidth]{img/vae_graphical_model_01.png}
  };
  \node [xshift=-5.75cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [Kingma \& Welling 2014; Rezende+ 2014]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}
\frametitle{Example: Variational Auto-Encoder for Binarized MNIST}
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=0.5cm, yshift=-1.0cm, anchor=north west] at (current page.north west) {
   \includegraphics[width=0.18\columnwidth]{img/vae_graphical_model.png}
  };
  \node [xshift=2.6cm, yshift=-1.7cm, anchor=north west] at (current page.north west) {
\includegraphics[height=0.14\textheight]{img/vae_decoder.png}
  };
  \node [xshift=2.6cm, yshift=-5.8cm, anchor=north west] at (current page.north west) {
\includegraphics[height=0.18\textheight]{img/vae_encoder.png}
  };
  \node [xshift=-5.75cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [Kingma \& Welling 2014; Rezende+ 2014]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}
\frametitle{Inference}
Given
\begin{itemize}
\item Data $\mbx_{\text{train}}$.
\item
Model $p(\mbx, \mbz, \mbbeta)$ of
observed variables $\mbx$ and latent variables $\mbz, \mbbeta$.
\end{itemize}
Goal
\begin{itemize}
\item
Calculate posterior distribution
\begin{equation*}
p(\mbz, \mbbeta\mid\mbx_{\text{train}}) =
\frac{p(\mbx_{\text{train}}, \mbz, \mbbeta)}{\int
p(\mbx_{\text{train}}, \mbz, \mbbeta) \d\mbz\d\mbbeta}.
\end{equation*}
\end{itemize}
\vspace{2ex}
This is the key problem in Bayesian inference.
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=-4.5cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\small \url{edwardlib.org/tutorials}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}
\frametitle{Inference}

In Edward, all inference has the same structure.

\begin{center}
\vspace{-2.0ex}
\includegraphics[height=0.05\textheight]{img/inference.png}
\end{center}

\texttt{Inference} has two inputs: \\
\begin{enumerate}
\vspace{-3ex}
\item
latent variables $\mbz,\beta$, binding model variables to approximate factors;
\item
observed variables $\mbx$, binding model variables to data.
\end{enumerate}

\texttt{Inference} has class methods:
\begin{itemize}
\item
\texttt{run()} runs the algorithm from initialization to convergence;
\item
\texttt{initialize()}, \texttt{update()}, \texttt{print\_progress()}, etc.
provides finer control.
\end{itemize}
\end{frame}

\begin{frame}[t]
\frametitle{Example: Variational Auto-Encoder for Binarized MNIST}
\vspace{5ex}
\begin{center}
\includegraphics[width=0.975\textwidth]{img/vae_example.png}
\end{center}
\end{frame}

\begin{frame}[t]
\frametitle{Example: Variational Auto-Encoder for Binarized MNIST}
\vspace{2.5ex}
\begin{center}
\includegraphics[width=1.05\textwidth]{img/vae_example_hmc.png}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Non-Bayesian Methods: GANs}
GANs posit a generative process,
\begin{align*}
\mbepsilon &\sim \text{Normal}(0, 1) \\
\mbx &= G(\mbepsilon;\theta)
\end{align*}
for some generative network $G$.

Training uses a discriminative network $D$ via the
optimization problem
\begin{equation*}
\min_\theta \max_\phi
\mathbb{E}_{p^*(\mbx)} [ \log D(\mbx; \phi) ]
+ \mathbb{E}_{p(\mbx; \theta)} [ \log (1 - D(\mbx; \phi)) ]
\end{equation*}
The generator tries to generate samples indistinguishable from true data.

The discriminator tries to discriminate samples from the generator and
samples from the true data.

\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=-3.0cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [Goodfellow+ 2014]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}
\frametitle{Non-Bayesian Methods: GANs}
\begin{center}
\vspace{-2ex}
\includegraphics[width=1.0\textwidth]{img/gan_example.png}
\end{center}
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=-3.0cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [Goodfellow+ 2014]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}
\frametitle{Non-Bayesian Methods: GANs}
\begin{center}
\vspace{-2ex}
\includegraphics[width=1.1\textwidth]{img/ganw_example.png}
\end{center}
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=-4.5cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [Arjovsky+ 2017; Gulrajani+ 2017]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}
\frametitle{Experiment: GPU-accelerated Hamiltonian Monte Carlo}
\begin{center}
\includegraphics[width=0.85\textwidth]{img/experiments_hmc.png}
\end{center}
\vspace{-1ex}
Run HMC for 100 iterations and fixed hyperparameters.

Bayesian logistic regression for Covertype ($581012$ data points, $54$
features).

12-core Intel i7-5930K CPU at 3.50GHz and NVIDIA Titan X (Maxwell) GPU.
Single precision.

\vspace{4ex}
\textbf{Edward is orders of magnitude faster than existing software
for large data.}
\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=-2cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [Tran+ 2017]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}
\frametitle{Experiment: Neural Machine Translation (EN-DE)}

\begin{table}
\begin{center}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{Method} &
  \multicolumn{5}{c}{\# of Supervised Examples (BLEU)} \\
  & 500K & 1M & 2M \\
\toprule
Transformer (65M params) &
12.3 & 14.6 & 21.3 \\
Transformer (~130M params) &
10.3 & 19.5 & 22.7 \\
\bf Feature matching (130M params) &
\bf 18.5 &\bf 22.4 &\bf 24.0 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}
\vspace{1ex}

WMT 2014 English-to-German ($\approx$4.5M sentences, 222M tokens).
Training set is $N$ parallel sentences, rest are non-parallel.

Test on newstest2014; validation on newstest2013.

SOTA is \gray{[Vaswani+ 2017]} (28.4);
Conv Seq2Seq (25.16).

SOTA 2016 is ByteNet (23.75);
Google NMT (24.6).

SOTA 2015 is RNN Enc-Dec-Att (20.9); RNN Enc-Dec (14.0).

\begin{tikzpicture}[remember picture,overlay]
  \node [xshift=-2.5cm, yshift=0.4cm, anchor=south west] at (current
  page.south east) {
\gray{\small [in preparation]}
  };
\end{tikzpicture}
\end{frame}

\begin{frame}[c]
\frametitle{Summary}
\begin{enumerate}
\item
Edward is a probabilistic programming system designed for fast
experimentation.

It emphasizes fine tuning of inference alongside model development.
\item
Edward is integrated into TensorFlow.

It features significant speedups over existing systems on large data
or multi-device / multi-machine environments.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Current directions}

We are extending Edward with new semantics.
\begin{enumerate}
\item
Distributed probabilistic programming.
\gray{(Google)}
\item
Idiomatic probabilistic programming with TensorFlow.
\gray{(Google)}
\item
A one-line API for loading standard data sets in machine learning.
\end{enumerate}
\vspace{3ex}

We are applying Edward for AI and scientific research.
\begin{enumerate}
\item
Causal models for genome wide association studies. \gray{(Blei)}
\item
Alignment \& data efficiency in language.
\gray{(OpenAI)}
\item
AGI (Solomonoff induction) by learning probabilistic programs.
\gray{(OpenAI)}
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{References}
\vspace{-1.5ex}
\begin{center}
\includegraphics[width=0.3\textwidth]{img/edward.png}
\\
\large \url{edwardlib.org}
\end{center}
\vspace{1ex}

Two NIPS workshops this year:
\begin{itemize}
\item
Approximate Bayesian Inference
(\url{approximateinference.org}).
\item
Bayesian Deep Learning
(\url{bayesiandeeplearning.org}).
\end{itemize}

Two NIPS papers this year:
\begin{itemize}
\item
\textbf{D.~Tran}, R.~Ranganath, and D.M.~Blei. \\
Deep and hierarchical implicit models. \\
\item
A.~Dieng, \textbf{D.~Tran}, R.~Ranganath, and D.M.~Blei. \\
The $\chi$-divergence for approximate inference. \\
\end{itemize}
\end{frame}

\end{document}
